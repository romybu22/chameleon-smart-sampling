{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup (only required for the first run on the Spark cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\operi157093\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (0.25.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\operi157093\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from pandas) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\operi157093\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from pandas) (2.8.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\operi157093\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from pandas) (1.16.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\operi157093\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from python-dateutil>=2.6.1->pandas) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'gsutil' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "ROOT_PATH = 'gs://news_public_datasets4/adressa'\n",
    "ROOT_PATH = 'C:\\\\Users\\\\operi157093\\\\Desktop\\\\Master\\\\First Year\\\\Semester B\\\\Recommendation Systems\\\\Assignments\\\\Project\\\\'\n",
    "#TODO: Upload this file (generated by the ACR module training) to GCS before calling spark script\n",
    "!gsutil cp {ROOT_PATH}/data_transformed/adressa_articles.csv ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Spark\\spark-3.0.0-bin-hadoop2.7\n",
      "C:\\Program Files\\Java\\jdk-11.0.7\n",
      "C:\\Users\\operi157093\\AppData\\Local\\Continuum\\anaconda3;C:\\Users\\operi157093\\AppData\\Local\\Continuum\\anaconda3\\Library\\mingw-w64\\bin;C:\\Users\\operi157093\\AppData\\Local\\Continuum\\anaconda3\\Library\\usr\\bin;C:\\Users\\operi157093\\AppData\\Local\\Continuum\\anaconda3\\Library\\bin;C:\\Users\\operi157093\\AppData\\Local\\Continuum\\anaconda3\\Scripts;C:\\Users\\operi157093\\AppData\\Local\\Continuum\\anaconda3\\bin;C:\\Users\\operi157093\\AppData\\Local\\Continuum\\anaconda3\\condabin;C:\\Users\\operi157093\\AppData\\Local\\Continuum\\anaconda3;C:\\Users\\operi157093\\AppData\\Local\\Continuum\\anaconda3\\Library\\mingw-w64\\bin;C:\\Users\\operi157093\\AppData\\Local\\Continuum\\anaconda3\\Library\\usr\\bin;C:\\Users\\operi157093\\AppData\\Local\\Continuum\\anaconda3\\Library\\bin;C:\\Users\\operi157093\\AppData\\Local\\Continuum\\anaconda3\\Scripts;C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\javapath;C:\\Program Files (x86)\\RSA SecurID Token Common;C:\\Program Files\\RSA SecurID Token Common;C:\\ProgramData\\Oracle\\Java\\javapath;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0;C:\\Program Files (x86)\\WebEx\\Productivity Tools;C:\\Program Files\\MATLAB\\R2012a\\runtime\\win64;C:\\Program Files\\MATLAB\\R2012a\\bin;C:\\Program Files\\PuTTY;C:\\Program Files (x86)\\Webex\\Webex\\Applications;C:\\Users\\Anaconda3\\Scripts;C:\\Users\\Anaconda3;C:\\Users\\Anaconda3\\Library\\bin;C:\\Spark\\spark-3.0.0-bin-hadoop2.7\\bin;C:\\Program Files\\Java\\jdk-11.0.7\\bin;C:\\Users\\operi157093\\AppData\\Local\\Programs\\Python\\Python37\\Scripts;C:\\Users\\operi157093\\AppData\\Local\\Programs\\Python\\Python37\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.environ['SPARK_HOME'])\n",
    "print(os.environ['JAVA_HOME'])\n",
    "print(os.environ['PATH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "\n",
    "import hashlib\n",
    "import math\n",
    "import matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.functions import PandasUDFType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading articles pre-processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_original_df = pd.read_csv(ROOT_PATH + 'adressa_articles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['adressa-access', 'author_1st', 'category0', 'category1', 'category2',\n",
       "       'concepts', 'created_at_ts', 'entities', 'id', 'keywords', 'locations',\n",
       "       'persons', 'publishtime', 'site', 'text_highlights', 'url',\n",
       "       'id_encoded', 'category0_encoded', 'category1_encoded',\n",
       "       'author_encoded', 'keywords_encoded', 'concepts_encoded',\n",
       "       'entities_encoded', 'locations_encoded', 'persons_encoded'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_original_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73309"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_original_df['url'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73309"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_articles_urls_to_ids_dict = dict(articles_original_df[['url','id_encoded']].apply(lambda x: (x['url'], x['id_encoded']), axis=1).values)\n",
    "len(valid_articles_urls_to_ids_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading user interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading interaction files: ['C:\\\\Users\\\\operi157093\\\\Desktop\\\\Master\\\\First Year\\\\Semester B\\\\Recommendation Systems\\\\Assignments\\\\Project\\\\one_week.tar/one_week/20170106']\n"
     ]
    }
   ],
   "source": [
    "#INTERACTIONS_PATH = 'gs://news_public_datasets4/adressa/one_week/*'\n",
    "#INTERACTIONS_PATH = 'three_month/20170101'\n",
    "from pyspark.shell import spark\n",
    "\n",
    "DAYS_TO_LOAD_INTERACTIONS=17\n",
    "#interaction_json_files = [os.path.join(ROOT_PATH, 'three_month/201701{:02d}'.format(day)) for day in range(1, DAYS_TO_LOAD_INTERACTIONS)]\n",
    "interaction_json_files = [os.path.join(ROOT_PATH, 'one_week.tar/one_week/20170106')]\n",
    "print('Loading interaction files: {}'.format(interaction_json_files))\n",
    "\n",
    "interactions_df = spark.read \\\n",
    "  .option(\"mode\", \"PERMISSIVE\") \\\n",
    "  .json(interaction_json_files)\n",
    "\n",
    "print(\"Done reading\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrives article id from its cannonical URL (because sometimes article ids in interactions do no match with articles tables, but cannonical URL do)\n",
    "def get_article_id_encoded_from_url(canonical_url):\n",
    "    if canonical_url in valid_articles_urls_to_ids_dict:\n",
    "        return valid_articles_urls_to_ids_dict[canonical_url]    \n",
    "    return None\n",
    "\n",
    "get_article_id_encoded_from_url_udf = F.udf(get_article_id_encoded_from_url, pyspark.sql.types.IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering only interactions whose url/id is available in the articles table\n",
    "interactions_article_id_encoded_df = interactions_df.withColumn('article_id', get_article_id_encoded_from_url_udf(interactions_df['canonicalUrl']))\n",
    "interactions_filtered_df = interactions_article_id_encoded_df.filter(interactions_article_id_encoded_df['article_id'].isNull() == False).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Valid interactions\n",
    "interactions_filtered_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distinct items count\n",
    "interactions_filtered_df.select('article_id').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_timestamp_ts = interactions_filtered_df.select('time').agg(F.min('time')).collect()[0][0] * 1000\n",
    "first_timestamp_ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing elapsed time since publishing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#interactions_filtered_df.filter(interactions_filtered_df['time'].isNull()).count()\n",
    "#0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timestamp_from_date_str(value):\n",
    "    if value is not None:\n",
    "        value = str(value)\n",
    "          #return int(datetime.datetime.strptime(value, '%Y-%m-%dT%H:%M:%S.%fZ').timestamp())\n",
    "        return int(datetime.datetime.strptime(value, '%Y-%m-%d %H:%M:%S').timestamp())\n",
    "    return None\n",
    "\n",
    "get_timestamp_from_date_str_udf = F.udf(get_timestamp_from_date_str, pyspark.sql.types.IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_filtered_with_publish_ts_df = interactions_filtered_df.withColumn('publish_ts', get_timestamp_from_date_str_udf(interactions_filtered_df['publishtime']))\n",
    "interactions_filtered_with_publish_ts_df = interactions_filtered_with_publish_ts_df.withColumn('elapsed_min_since_published', ((F.col('time') - F.col('publish_ts')) / 60).cast(pyspark.sql.types.IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#interactions_filtered_with_publish_ts_df.select('publishtime','publish_ts', 'time', 'elapsed_min_since_published').show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "interactions_filtered_with_publish_ts_df.approxQuantile(\"elapsed_min_since_published\", [0.10, 0.25, 0.50, 0.75, 0.90], 0.01)\n",
    "#[49.0, 108.0, 334.0, 1020.0, 4611.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elapsed_min_since_published_df = interactions_filtered_with_publish_ts_df.select('elapsed_min_since_published').toPandas()\n",
    "print(len(elapsed_min_since_published_df[pd.isnull(elapsed_min_since_published_df['elapsed_min_since_published'])]))\n",
    "elapsed_min_since_published_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "elapsed_min_since_published\n",
    "count\t2.600818e+06\n",
    "mean\t6.438622e+04\n",
    "std\t5.051825e+05\n",
    "min\t-3.151590e+05\n",
    "25%\t9.400000e+01\n",
    "50%\t2.580000e+02\n",
    "75%\t8.370000e+02\n",
    "max\t8.608278e+06\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing clicks by article distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clicks_by_article_count_df = interactions_filtered_df.groupBy('article_id').count()\n",
    "#clicks_by_article_count_df.approxQuantile(\"count\", [0.01, 0.10, 0.25, 0.50, 0.75, 0.90, 0.99], 0.01)\n",
    "#[1.0, 1.0, 1.0, 1.0, 2.0, 6.0, 33581.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categ_features_counts_dataframe(interactions_spark_df,column_name):\n",
    "    df_pandas = interactions_spark_df.groupBy(column_name).count().toPandas().sort_values('count', ascending=False)\n",
    "    return df_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_TOKEN = '<PAD>'\n",
    "UNFREQ_TOKEN = '<UNF>'\n",
    "\n",
    "def get_encoder_for_values(values):\n",
    "    encoder_values = [PAD_TOKEN, UNFREQ_TOKEN] + values\n",
    "    encoder_ids = list(range(len(encoder_values)))\n",
    "    encoder_dict = dict(zip(encoder_values, encoder_ids))\n",
    "    return encoder_dict\n",
    "\n",
    "def get_categ_features_encoder_dict(counts_df, min_freq=100):\n",
    "    freq_values = counts_df[counts_df['count'] >= 100][counts_df.columns[0]].values.tolist()\n",
    "    encoder_dict = get_encoder_for_values(freq_values)\n",
    "    return encoder_dict\n",
    "\n",
    "def encode_cat_feature(value, encoder_dict):\n",
    "    if value in encoder_dict:\n",
    "        return encoder_dict[value]\n",
    "    else:\n",
    "        return encoder_dict[UNFREQ_TOKEN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_df = get_categ_features_counts_dataframe(interactions_filtered_df, 'country')\n",
    "len(countries_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_encoder_dict = get_categ_features_encoder_dict(countries_df)\n",
    "len(countries_encoder_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_df = get_categ_features_counts_dataframe(interactions_filtered_df, 'city')\n",
    "len(cities_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_encoder_dict = get_categ_features_encoder_dict(cities_df)\n",
    "len(cities_encoder_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions_df = get_categ_features_counts_dataframe(interactions_filtered_df, 'region')\n",
    "len(regions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions_encoder_dict = get_categ_features_encoder_dict(regions_df)\n",
    "len(regions_encoder_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "devices_df = get_categ_features_counts_dataframe(interactions_filtered_df, 'deviceType')\n",
    "print(len(devices_df))\n",
    "devices_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "devices_encoder_dict = get_categ_features_encoder_dict(devices_df)\n",
    "len(devices_encoder_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os_df = get_categ_features_counts_dataframe(interactions_filtered_df, 'os')\n",
    "print(len(os_df))\n",
    "os_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os_encoder_dict = get_categ_features_encoder_dict(os_df)\n",
    "len(os_encoder_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "referrer_class_df = get_categ_features_counts_dataframe(interactions_filtered_df, 'referrerHostClass')\n",
    "print(len(referrer_class_df))\n",
    "referrer_class_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "referrer_class_encoder_dict = get_categ_features_encoder_dict(referrer_class_df)\n",
    "len(referrer_class_encoder_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoders_dict = {\n",
    "    'city': cities_encoder_dict,\n",
    "    'region': regions_encoder_dict,\n",
    "    'country': countries_encoder_dict,\n",
    "    'os': os_encoder_dict,\n",
    "    'device': devices_encoder_dict,\n",
    "    'referrer_class': referrer_class_encoder_dict\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "active_time_quantiles = interactions_filtered_df.approxQuantile(\"activeTime\", [0.10, 0.25, 0.50, 0.75, 0.90], 0.01)\n",
    "print(active_time_quantiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_time_stats_df = interactions_filtered_df.describe('activeTime').toPandas()\n",
    "active_time_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_time_mean = float(active_time_stats_df[active_time_stats_df['summary'] == 'mean']['activeTime'].values[0])\n",
    "active_time_stddev = float(active_time_stats_df[active_time_stats_df['summary'] == 'stddev']['activeTime'].values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "schema = T.StructType([\n",
    "    T.StructField(\"userId\", T.StringType()),\n",
    "    T.StructField(\"min_ts\", T.IntegerType())\n",
    "])\n",
    "\n",
    "@pandas_udf(schema, functionType=PandasUDFType.GROUPED_MAP)\n",
    "def split_sessions(df):\n",
    "\n",
    "    result_df = df[['userId']]\n",
    "    result_df['min_ts'] = df['time'].min()\n",
    "    \n",
    "    return result\n",
    "\n",
    "%%time\n",
    "tmp = interactions_filtered_df.groupBy('userId').apply(split_sessions)\n",
    "tmp.show(100)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_str_to_int(encoded_bytes_text, digits):\n",
    "    return int(str(int(hashlib.md5(encoded_bytes_text).hexdigest()[:8], 16))[:digits])      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SESSION_IDLE_TIME_MS = 30 * 60 * 1000    #30 min\n",
    "\n",
    "def close_session(session):\n",
    "    size = len(session)\n",
    "    \n",
    "    #Creating and artificial session id based on the first click timestamp and a hash of user id\n",
    "    first_click = session[0]\n",
    "    session_id = (int(first_click['timestamp']) * 100) + hash_str_to_int(first_click['user_id'].encode(), 3)\n",
    "    session_hour = int((first_click['timestamp'] - first_timestamp_ts) / (1000 * 60 * 60)) #Converting timestamp to hours since first timestamp\n",
    "    \n",
    "    #Converting to Spark DataFrame Rows, to convert RDD back to DataFrame\n",
    "    clicks = list([T.Row(**click) for click in session])\n",
    "    session_dict = {'session_id': session_id,\n",
    "                    'session_hour': session_hour,\n",
    "                    'session_size': size,\n",
    "                    'session_start': first_click['timestamp'],\n",
    "                    'user_id': first_click['user_id'],\n",
    "                    'clicks': clicks \n",
    "                   }\n",
    "    session_row = T.Row(**session_dict)\n",
    "    \n",
    "    return session_row\n",
    "        \n",
    "def transform_interaction(interaction):        \n",
    "    return {\n",
    "            'article_id': interaction['article_id'],\n",
    "            'url': interaction['canonicalUrl'],\n",
    "            'user_id': interaction['userId'],\n",
    "            'timestamp': interaction['time'] * 1000, #converting to timestamp\n",
    "            'active_time_secs': interaction['activeTime'],\n",
    "            'country': encode_cat_feature(interaction['country'], encoders_dict['country']),\n",
    "            'region': encode_cat_feature(interaction['region'], encoders_dict['region']),\n",
    "            'city': encode_cat_feature(interaction['city'], encoders_dict['city']),\n",
    "            'os': encode_cat_feature(interaction['os'], encoders_dict['os']),\n",
    "            'device': encode_cat_feature(interaction['deviceType'], encoders_dict['device']),\n",
    "            'referrer_class': encode_cat_feature(interaction['referrerHostClass'], encoders_dict['referrer_class']),\n",
    "           }\n",
    "\n",
    "def split_sessions(group):\n",
    "    user, interactions = group\n",
    "    #Ensuring items are sorted by time\n",
    "    interactions_sorted_by_time = sorted(interactions, key=lambda x: x['time'])\n",
    "    #Transforming interactions\n",
    "    interactions_transformed = list(map(transform_interaction, interactions_sorted_by_time))\n",
    "\n",
    "    \n",
    "    sessions = []\n",
    "    session = []        \n",
    "    first_timestamp = interactions_transformed[0]['timestamp']\n",
    "    last_timestamp = first_timestamp    \n",
    "    for interaction in interactions_transformed:\n",
    "        \n",
    "        delta_ms = (interaction['timestamp'] - last_timestamp)\n",
    "        interaction['_elapsed_ms_since_last_click'] = delta_ms \n",
    "\n",
    "        if delta_ms <= MAX_SESSION_IDLE_TIME_MS:    \n",
    "            #Ignoring repeated items in session\n",
    "            if len(list(filter(lambda x: x['article_id'] == interaction['article_id'], session))) == 0:        \n",
    "                session.append(interaction)            \n",
    "        else:\n",
    "            #If session have at least 2 clicks (minimum for next click predicition)\n",
    "            if len(session) >= 2:\n",
    "                session_row = close_session(session)\n",
    "                sessions.append(session_row)                \n",
    "            session = [interaction]\n",
    "\n",
    "        last_timestamp = interaction['timestamp']\n",
    "            \n",
    "    if len(session) >= 2:\n",
    "        session_row = close_session(session)\n",
    "        sessions.append(session_row)\n",
    "        \n",
    "    #if len(sessions) > 1:\n",
    "    #    raise Exception('USER with more than one session: {}'.format(user))\n",
    "    \n",
    "    return list(zip(map(lambda x: x['session_id'], sessions), \n",
    "                    sessions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#To debug\n",
    "%%time\n",
    "sessions_rdd = interactions_filtered_df.limit(1000).rdd.map(lambda x: (x['userId'], x)).groupByKey() \\\n",
    "                    .collect()\n",
    "\n",
    "for row in sessions_rdd:\n",
    "    print(split_sessions(row))\n",
    "    print()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sessions_rdd = interactions_filtered_df.rdd.map(lambda x: (x['userId'], x)).groupByKey() \\\n",
    "                            .flatMap(split_sessions) \\\n",
    "                            .sortByKey() \\\n",
    "                            .map(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exporting sessions to JSON lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions_sdf = sessions_rdd.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sessions_sdf.write.partitionBy(\"session_hour\").json(os.path.join(ROOT_PATH,\"data_transformed/sessions_processed_by_spark/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions_sdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize(filename, obj):\n",
    "    with open(filename, 'wb') as handle:\n",
    "        pickle.dump(obj, handle)#, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAR_ENCODERS_PATH  = 'nar_encoders_adressa.pickle'\n",
    "serialize(NAR_ENCODERS_PATH, encoders_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp {NAR_ENCODERS_PATH} {ROOT_PATH}/data_transformed/pickles/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
